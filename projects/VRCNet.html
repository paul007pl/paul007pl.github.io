<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Variational Relational Point Completion Network</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Real-scanned point clouds are often incomplete due to viewpoint, occlusion, and noise.  Existing point cloud completion methods tend to generate global shape skeletons and hence lack fine local details.  Furthermore, they mostly learn a deterministic partial-to-complete mapping, but overlook structural relations in man-made objects.  To tackle these challenges, this paper proposes a variational framework, Variational Relational point Completion network (VRCNet) with two appealing properties: 1) Probabilistic Modeling. In particular, we propose a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds.  One path consumes complete point clouds for reconstruction by learning a point VAE.  The other path generates complete shapes for partial point clouds, whose embedded distribution is guided by distribution obtained from the reconstruction path during training.  2) Relational Enhancement.  Specifically, we carefully design point self-attention kernel and point selective kernel module to exploit relational point features, which refines local shape details conditioned on the coarse completion.  In addition, we contribute a multi-view partial point cloud dataset (MVP dataset) containing over 100,000 high-quality scans, which renders partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model.  Extensive experiments demonstrate that VRCNet outperforms state-of-the-art methods on all standard point cloud completion benchmarks.  Notably, VRCNet shows great generalizability and robustness on real-world point cloud scans.">
<meta name="keywords" content="point cloud completion; VAE; Self-Attention; MVP Dataset;">
<!-- <link rel="author" href="https://liuziwei7.github.io/"> -->

<!-- Fonts and stuff -->
<link href="./VRCNet/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./VRCNet/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./VRCNet/iconize.css">
<script async="" src="./VRCNet/prettify.js"></script>


</head>

<body>
	<div id="content">
		<div id="content-inner">
			
			<div class="section head">
	<h1>Variational Relational Point Completion Network</h1>

	<div class="authors_list1">
		<a href="https://github.com/paul007pl/">Liang Pan</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		Xinyi Chen<sup>1 2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		<a href="https://scholar.google.com/citations?hl=en&user=WrDKqIAAAAAJ&view_op=list_works&sortby=pubdate">Zhongang Cai</a><sup>2 3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		<a href="https://scholar.google.com.sg/citations?user=wupwVHAAAAAJ&hl=en">Junzhe Zhang</a><sup>1 2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		<br>
		<a href="https://scholar.google.com/citations?user=sMQV1ecAAAAJ">Haiyu Zhao</a><sup>2 3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		<a href="https://scholar.google.com/citations?user=afbbNmwAAAAJ&hl=en">Shuai Yi</a><sup>2 3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		<a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</div>

<!-- 	<div class="authors_list2">
		Haiyu Zhao<sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		Shuai Yi<sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		<a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</div> -->

	<div class="affiliations">
		<br>
		1. <a href="https://www.ntu.edu.sg/Pages/home.aspx/">S-Lab, Nanyang Technological University</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		2. <a href="https://www.sensetime.com/">SenseTime Research</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		3. <a href="https://www.shlab.org.cn/pc/home">Shanghai AI Laboratory</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</div>

	<div class="venue">Computer Vision and Pattern Recognition 2021 (<a href="https://tog.acm.org/" target="_blank"> CVPR oral </a>) </div>
			</div>

<div class="section">
	<br>
	<center>
		<img src="./VRCNet/intro.png" border="0" width="100%">
		<br>
		<p align="justify">
			&nbsp;&nbsp;&nbsp;&nbsp;<b>(a) VRCNet performs shape completion with two consecutive stages:</b> probabilistic modeling and relational enhancement.  <b>(b) Qualitative Results</b> show that VRCNet generates better shape details than the other works.  <b>(c) Our completion results conditioned on partial observations.</b> The arrows indicate the viewing angles. In (1) and (2), 2 knots are partially observed for the pole of the lamp, and hence we generate 2 complete knots. In (3), only 1 knot is observed, and then we reconstruct 1 complete knot. If no knots are observed (see (4)), VRCNet generates a smooth pole without knots.
		</p>
	</center>
</div>

<div class="section news">
	<h2>News</h2>
	<p><strong>2021-07-12</strong> <a href="https://competitions.codalab.org/competitions/33430" target="_blank"><b>The submission on Codalab starts!</b></a><img src = './VRCNet/new.jpg' width = '28'></p>
	<p><strong>2021-07-10</strong> Two benchmarks, <a href="https://mvp-dataset.github.io/MVP/Completion.html" target="_blank"><b>Single-View Point Cloud Completion</b></a> and 
		<a href="https://mvp-dataset.github.io/MVP/Registration.html" target="_blank"><b>Partial-to-Partial Point Cloud Registration</b></a> 
		based on the <a href="https://mvp-dataset.github.io/"><b>MVP database</b></a> have been released. <img src = './VRCNet/new.jpg' width = '28'></p>
	<p><strong>2021-07-09</strong> An open-source toolbox for Point Cloud Completion and Registration, <a href="https://github.com/paul007pl/MVP_Benchmark"><b>MVP codebase</b></a>, has been released. <img src = './VRCNet/new.jpg' width = '28'></p>
	<p><strong>2021</strong> The MVP challenges will be hosted in the <a href="https://sense-human.github.io"><b>ICCV2021 Workshop: Sensing, Understanding and Synthesizing Humans.</b></a></p>
  </div>	

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p align="justify">
		&nbsp;&nbsp;&nbsp;&nbsp;Real-scanned point clouds are often incomplete due to viewpoint, occlusion, and noise.  Existing point cloud completion methods tend to generate global shape skeletons and hence lack fine local details.  Furthermore, they mostly learn a deterministic partial-to-complete mapping, but overlook structural relations in man-made objects.  To tackle these challenges, this paper proposes a variational framework, Variational Relational point Completion network (VRCNet) with two appealing properties: 1) Probabilistic Modeling. In particular, we propose a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds.  One path consumes complete point clouds for reconstruction by learning a point VAE.  The other path generates complete shapes for partial point clouds, whose embedded distribution is guided by distribution obtained from the reconstruction path during training.  2) Relational Enhancement.  Specifically, we carefully design point self-attention kernel and point selective kernel module to exploit relational point features, which refines local shape details conditioned on the coarse completion.  In addition, we contribute a multi-view partial point cloud dataset (MVP dataset) containing over 100,000 high-quality scans, which renders partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model.  Extensive experiments demonstrate that VRCNet outperforms state-of-the-art methods on all standard point cloud completion benchmarks.  Notably, VRCNet shows great generalizability and robustness on real-world point cloud scans.
	</p>
</div>

<br>

<div class="section highlight">
	<h2>Method Highlight</h2>
	<br>
	<center>
		<!-- <br> -->
		<img src="./VRCNet/point_modules.png" border="0" width="100%">
		<p align="justify">
			&nbsp;&nbsp;&nbsp;&nbsp; <b>Our proposed point kernels.</b> (a) Our PSA adaptively aggregate neighboring point features. (b) Using selective kernel unit, our PSK can adaptively adjust receptive fields to exploit and fuse multi-scale point features. (c) By adding a	residual connection, we construct our RPSK that is an important building block for our RENet.
		</p>
	</center>
</div>

<!-- <div class="section Paper, Code & Dataset">
	<h2>Paper, Code & Dataset</h2>
		<center>
			<ul>
				<li class="grid">
					<div class="griditem">
						<a href="https://arxiv.org/abs/1801.07829" target="_blank" class="imageLink"><img src="./VRCNet/paper.jpg"></a><br>
						<a href="https://arxiv.org/abs/1801.07829" target="_blank">Paper</a>
					</div>
				</li>
			</ul>
			Coming soonâ€¦
		</center>
</div>
<br>  -->

<br>

<div class="section mvp">
	<h2>MVP Dataset</h2>
	<br>
	<center>
		<a href="https://www.dropbox.com/sh/la0kwlqx4n2s5e3/AACjoTzt-_vlX6OF9mfSpFMra?dl=0&lst=" target="_blank" class="imageLink"><img src="./VRCNet/mvp.png" border="0" width="100%"></a>
		<br>
		<p align="justify">
			&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.dropbox.com/sh/la0kwlqx4n2s5e3/AACjoTzt-_vlX6OF9mfSpFMra?dl=0&lst=" target="_blank"><b>Multi-View Partial (MVP) Point Cloud Dataset.</b></a> <b>(a)</b> shows an example for our 26 uniformly distributed camera poses on a unit sphere. <b>(b)</b> presents the 26 partial point clouds for the airplane from our uniformly distributed virtual cameras. <b>(c)</b> compares the rendered incomplete point clouds with different camera resolutions. <b>(d)</b> shows that Poisson disk sampling
			generates complete point clouds with a higher quality than uniform sampling. <b> Download: </b> <a href="https://drive.google.com/drive/folders/1ylC-dYFM45KW4K9tPyljBSVyetazCEeH?usp=sharing" target="_blank"> [Google Drive]</a>, <a href="https://www.dropbox.com/sh/la0kwlqx4n2s5e3/AACjoTzt-_vlX6OF9mfSpFMra?dl=0&lst=" target="_blank"> [Dropbox]</a>.
		</p>
	</center>
</div>
			
<br>

<div class="section materials">
	<h2>Materials</h2>
	<table width="100%" align="center" border="none" cellspacing="0" cellpadding="30">
          <tbody><tr>
            <td width="60%">
              <center>
                <a href="https://arxiv.org/abs/2104.10154" target="_blank" class="imageLink"><img src="./VRCNet/paper_pages.png" ,="" width="52%"></a><br><br>
                <a href="https://arxiv.org/abs/2104.10154" target="_blank">Paper</a>
              </center>
            </td>
            <td width="40%" valign="middle">
              <center>
                <a href="https://github.com/paul007pl/VRCNet" target="_blank" class="imageLink"><img src="./VRCNet/icon_github.png" ,="" width="50%"></a><br><br>
                <a href="https://github.com/paul007pl/VRCNet" target="_blank">Code</a>
              </center>
            </td>
          </tr>
        </tbody></table>
</div>
			
<br>

<div class="section video">
	<h2>Videos</h2>
	<br>
	<table width="100%" align="center" border="none" cellspacing="0" cellpadding="30">
          <tbody><tr>
            <td width="40%" valign="middle">
              <center>
                <iframe width="400" height="225" src="https://www.youtube.com/embed/8qyhsyis9JY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                <br><br>
                <a href="https://www.youtube.com/watch?v=8qyhsyis9JY" target="_blank"> Presentation</a>
              </center>
            </td>
            <td width="40%" valign="middle">
              <center>
                <iframe width="400" height="225" src="https://www.youtube.com/embed/0SNHlxvCP0g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                <br><br>
                <a href="https://www.youtube.com/watch?v=0SNHlxvCP0g" target="_blank"> More Qualitative Results</a>
              </center>
            </td>
          </tr>
        </tbody></table>
	<!-- <center> -->
		<!-- <ul> -->
			<!-- <li class="grid"> -->
				<!-- <div class="griditem"> -->
					<!-- <a href="https://www.youtube.com/watch?v=0SNHlxvCP0g" target="_blank" class="imageLink"><img src="./VRCNet/video_clip.png" border="0" width="40%"></a> -->
					<!-- <iframe width="439" height="247" src="https://www.youtube.com/embed/8qyhsyis9JY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
					<!-- <iframe width="439" height="247" src="https://www.youtube.com/embed/0SNHlxvCP0g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
					<!-- <br><br> -->
					<!-- <a href="https://www.youtube.com/watch?v=8qyhsyis9JY" target="_blank"> Presentation</a> -->
					<!-- <a href="https://www.youtube.com/watch?v=0SNHlxvCP0g" target="_blank"> More Qualitative Results</a> -->
				<!-- </div> -->
			<!-- </li> -->
		<!-- </ul> -->
	<!-- </center> -->
</div>


<!-- <div class="section MVP dataset">
	<h2>MVP Dataset</h2>
	<center>
		<ul>
			<li class="grid">
				<div class="griditem">
					<a target="_blank" class="imageLink"><img src="./VRCNet/mvp.png"></a><br>
					<a target="_blank">Multi-View Partial (MVP) Point Cloud Dataset</a>
				</div>
			</li>
		</ul>
	</center>
</div>

<br> -->

<!-- <div class="section materials">
	<h2>Materials</h2>
	<center>
		<ul>
					 
					<li class="grid">
				<div class="griditem">
		<a href="https://arxiv.org/abs/1801.07829" target="_blank" class="imageLink"><img src="./VRCNet/paper_pages.png"></a><br>
			<a href="https://arxiv.org/abs/1801.07829" target="_blank">Paper</a>
		</div>
				</li>
		
			</ul>
			</center>
			</div>
			
<br> -->

<!-- <div class="section code">
	<h2>Code and Models</h2>
	<center>
		<ul>
					 
					<li class="grid">
				<div class="griditem">
		<a href="https://github.com/paul007pl/VRCNet" target="_blank" class="imageLink"><img src="./VRCNet/code.png"></a><br>
			<a href="https://github.com/paul007pl/VRCNet" target="_blank">Code and Models</a>
		</div>
				</li>

			</ul>
			</center>
			</div>
			
<br>

<div class="section app">
	<h2>Real-world Applications</h2>
	<br>
	<center>
					 
		<a href="https://arxiv.org/abs/1902.08570" target="_blank" class="imageLink"><img src="./VRCNet/lhc.jpg" border="0" width="50%"></a><br>
			<a href="https://arxiv.org/abs/1902.08570" target="_blank">ParticalNet in Large Hadron Collider (LHC)</a>
	
			</center>
			</div>
			
<br>

<div class="section media">
	<h2>Media Coverage</h2>
	<br>
	<center>
					 
		<a href="http://news.mit.edu/2019/deep-learning-point-clouds-1021" target="_blank" class="imageLink"><img src="./VRCNet/media.png" border="0" width="50%"></a><br>
			<a href="http://news.mit.edu/2019/deep-learning-point-clouds-1021" target="_blank">MIT News</a>
	
			</center>
			</div>
			
<br> -->

<div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
		<pre>@article{pan2021variational,
	title={Variational Relational Point Completion Network},
	author={Pan, Liang and Chen, Xinyi and Cai, Zhongang and Zhang, Junzhe and Zhao, Haiyu and Yi, Shuai and Liu, Ziwei},
	journal={arXiv preprint arXiv:2104.10154},
	year={2021}
}</pre>
	</div>
</div>

</body></html>