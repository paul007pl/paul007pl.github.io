<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Dynamic Graph CNN for Learning on Point Clouds</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Real-scanned point clouds are often incomplete due to viewpoint, occlusion, and noise.  Existing point cloud completion methods tend to generate global shape skeletons and hence lack fine local details.  Furthermore, they mostly learn a deterministic partial-to-complete mapping, but overlook structural relations in man-made objects.  To tackle these challenges, this paper proposes a variational framework, Variational Relational point Completion network (VRCNet) with two appealing properties: 1) Probabilistic Modeling. In particular, we propose a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds.  One path consumes complete point clouds for reconstruction by learning a point VAE.  The other path generates complete shapes for partial point clouds, whose embedded distribution is guided by distribution obtained from the reconstruction path during training.  2) Relational Enhancement.  Specifically, we carefully design point self-attention kernel and point selective kernel module to exploit relational point features, which refines local shape details conditioned on the coarse completion.  In addition, we contribute a multi-view partial point cloud dataset (MVP dataset) containing over 100,000 high-quality scans, which renders partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model.  Extensive experiments demonstrate that VRCNet outperforms state-of-the-art methods on all standard point cloud completion benchmarks.  Notably, VRCNet shows great generalizability and robustness on real-world point cloud scans.">
<meta name="keywords" content="point cloud; graph CNN; 3D vision; deep learning;">
<!-- <link rel="author" href="https://liuziwei7.github.io/"> -->

<!-- Fonts and stuff -->
<link href="./VRCNet/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./VRCNet/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./VRCNet/iconize.css">
<script async="" src="./VRCNet/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1>Variational Relational Point Completion Network</h1>

	<div class="authors">
	  <a href="https://people.csail.mit.edu/yuewang/">Yue Wang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://scholar.google.com/citations?user=pE_8JZ0AAAAJ">Yongbin Sun</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://meche.mit.edu/people/faculty/sesarma%40mit.edu">Sanjay E. Sarma</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://www.cs.technion.ac.il/~mbron/">Michael M. Bronstein</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://people.csail.mit.edu/jsolomon/">Justin M. Solomon</a><sup>1</sup>
	</div>

	<div class="affiliations">
	  1. <a href="http://web.mit.edu/">Massachusetts Institute of Technology</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  2. <a href="https://www.berkeley.edu/">UC Berkeley</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  3. <a href="https://www.usi.ch/">USI / TAU / Intel</a>
	</div>

	<div class="venue">ACM Transactions on Graphics (<a href="https://tog.acm.org/" target="_blank">TOG</a>) 2019</div>
      </div>

      
      <center><img src="./VRCNet/intro.png" border="0" width="90%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
Point clouds provide a flexible and scalable geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. Hence, the design of intelligent computational models that act directly on point clouds is critical, especially when efficiency considerations or noise preclude the possibility of expensive denoising and meshing procedures. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv is differentiable and can be plugged into existing architectures. Compared to existing modules operating largely in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked or recurrently applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. Beyond proposing this module, we provide extensive evaluation and analysis revealing that EdgeConv captures and exploits fine-grained geometric properties of point clouds. The proposed approach achieves state-of-the-art performance on standard benchmarks including ModelNet40 and S3DIS.
	</p>
      </div>
      
<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://arxiv.org/abs/1801.07829" target="_blank" class="imageLink"><img src="./VRCNet/paper.jpg"></a><br>
		  <a href="https://arxiv.org/abs/1801.07829" target="_blank">Paper</a>
		</div>
	      </li>
	  
	    </ul>
	    </center>
	    </div>
	    
<br>

<div class="section code">
	<h2>Code and Models</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/paul007pl/VRCNet" target="_blank" class="imageLink"><img src="./VRCNet/code.png"></a><br>
		  <a href="https://github.com/WangYueFt/VRCNet" target="_blank">Code and Models</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>
	    
<br>

<div class="section app">
	<h2>Real-world Applications</h2>
	<br>
	<center>
           
		<a href="https://arxiv.org/abs/1902.08570" target="_blank" class="imageLink"><img src="./VRCNet/lhc.jpg" border="0" width="50%"></a><br>
		  <a href="https://arxiv.org/abs/1902.08570" target="_blank">ParticalNet in Large Hadron Collider (LHC)</a>
	
	    </center>
	    </div>
	    
<br>

<div class="section media">
	<h2>Media Coverage</h2>
	<br>
	<center>
           
		<a href="http://news.mit.edu/2019/deep-learning-point-clouds-1021" target="_blank" class="imageLink"><img src="./VRCNet/media.png" border="0" width="50%"></a><br>
		  <a href="http://news.mit.edu/2019/deep-learning-point-clouds-1021" target="_blank">MIT News</a>
	
	    </center>
	    </div>
	    
<br>

<!-- <div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>@article{vrcnet,
  title={Dynamic Graph CNN for Learning on Point Clouds},
  author={Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
  journal={ACM Transactions on Graphics (TOG)},
  year={2019}
}</pre>
	  </div>
      </div> -->

</body></html>